{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import string\n",
    "import itertools\n",
    "#Included metrics are Levenshtein, Hamming, Jaccard, and Sorensen distance, plus some bonuses\n",
    "import distance\n",
    "import math\n",
    "\n",
    "# From configuration file\n",
    "from configparser import ConfigParser\n",
    "import codecs\n",
    "\n",
    "import collections\n",
    "\n",
    "from fastdtw import fastdtw # Dynamic time warping\n",
    "from IPython.display import clear_output\n",
    "# Shapely - for points and polygons (AOIS)\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "# Distances\n",
    "from scipy.spatial.distance import *\n",
    "from scipy import stats\n",
    "\n",
    "# Machine learning and clustering scitic\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.metrics import silhouette_samples, silhouette_score \n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing # predspracovanie dat - scale\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Custom modules\n",
    "from data_validation import data_validation_filter as dvf\n",
    "from data_processing import aoi as ac\n",
    "from data_processing import data_preprocessing as dp\n",
    "\n",
    "# RQA\n",
    "from algorithms.RQA import DynamicalSystemsModule as RecurrenceFunctions\n",
    "from algorithms.RQA import SpatioTemporalEyeTrackingModule as ReoccurrenceFunctions\n",
    "\n",
    "# Clustering from pyclustering\n",
    "import pyclustering.cluster.dbscan as cluster\n",
    "import pyclustering.cluster.kmedoids as kmedoid\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = [\"cyan\", \"yellow\",\"red\",\"blue\", \"green\",\"magenta\",\"black\",\"white\"]\n",
    "\n",
    "BOLD_START = '\\033[1m'\n",
    "BOLD_END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trieda Participant\n",
    "Reprezentacia participanta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Participant:\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.data = {}\n",
    "        self.data[\"Name\"] = name\n",
    "        self.data[\"Scanpath\"] = \"\"\n",
    "        self.data[\"ScanpathShort\"] = \"\"\n",
    "        self.data[\"NumberOfAoisHits\"] = 0\n",
    "        self.data[\"NumberOfAoisHitsShort\"] = 0\n",
    "        \n",
    "        self.data[\"NumberOfFixations\"] = 0\n",
    "        self.data[\"FixationsDuration\"] = 0\n",
    "        self.data[\"AverageFixationDuration\"] = 0\n",
    "        \n",
    "        # Saccades\n",
    "        # Average\n",
    "        self.data[\"AverageSaccadeSpeed\"] = 0\n",
    "        # Average\n",
    "        self.data[\"AverageSaccadeLength\"] = 0\n",
    "        \n",
    "        #RQA\n",
    "        # Reoccurrence\n",
    "        self.data[\"Reoccurrence\"] = 0\n",
    "        self.data[\"ReoccurrenceRate\"] = 0\n",
    "        self.data[\"ReoccurrenceDeterminism\"] = 0\n",
    "        self.data[\"ReoccurrenceLaminarity\"] = 0\n",
    "        self.data[\"ReoccurrenceCORM\"] = 0\n",
    "        # Recurrence\n",
    "        self.data[\"Recurrence\"] = 0\n",
    "        self.data[\"RecurrenceRate\"] = 0\n",
    "        self.data[\"RecurrenceMeanX\"] = 0\n",
    "        self.data[\"RecurrenceMeanY\"] = 0\n",
    "        self.data[\"RecurrenceStandardDeviationX\"] = 0\n",
    "        self.data[\"RecurrenceStandardDeviationY\"] = 0\n",
    "        \n",
    "        #DTW - Dynamic time warping average score (comparing to each scanpatch exlude itself)\n",
    "        self.data[\"DTW\"] = 0\n",
    "        \n",
    "        # LCS, LEV - Longest common subsequence, levensthein distance\n",
    "        self.data[\"MaxLcs\"] = 0\n",
    "        self.data[\"MeanLcs\"] = 0\n",
    "        self.data[\"MinLcs\"] = 0\n",
    "        \n",
    "        self.data[\"MaxLev\"] = 0\n",
    "        self.data[\"MeanLev\"] = 0\n",
    "        self.data[\"MinLev\"] = 0\n",
    "        \n",
    "        \n",
    "        #self.data[\"OutlierScore\"] = 0\n",
    "        \n",
    "        # For RQA\n",
    "        self.fixations = []\n",
    "        self.fixationsToAoi = []\n",
    "        self.outlierScore = 0\n",
    "        self.completeTask = False\n",
    "        self.passCalibration = False\n",
    "        self.passValidation = False\n",
    "        self.hasEnoughData = False\n",
    "\n",
    "        \n",
    "    def set_data(self, key, value):\n",
    "        self.data[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozne pomocne metody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_to_labels(X, clusters, noise = []):\n",
    "    labels = [0] * len(X)\n",
    "    for nois in noise:\n",
    "        labels[nois] = -1\n",
    "        \n",
    "    i = 0  \n",
    "    for one_cluster in clusters:\n",
    "        for one in one_cluster:\n",
    "            labels[one] = i\n",
    "        i += 1\n",
    "    \n",
    "    return np.array(labels, dtype=\"int64\")\n",
    "\n",
    "\n",
    "def pair_labels_with_participants_dict(labels, df_names):\n",
    "    participants_labels = {}\n",
    "    i = 0\n",
    "    for i in range(0,len(df_names)):\n",
    "        participants_labels[df_names[i]] = round(labels[i], 3)\n",
    "    return participants_labels\n",
    "\n",
    "def pair_scores_with_participants(scores, df_names):\n",
    "    participants_scores = []\n",
    "    #i = 0\n",
    "    for i in range(0,len(df_names)):\n",
    "        participants_scores.append([df_names[i], round(scores[i], 3)])\n",
    "    return participants_scores\n",
    "\n",
    "def pair_multiple_scores_with_participants(scores, df_names):\n",
    "    scores_together = np.zeros((len(df_names),len(scores)))\n",
    "    participants_scores = []\n",
    "    for i in range(0, len(scores)):\n",
    "        for j in range(0, len(scores[i])):\n",
    "            scores_together[j][i] = round(scores[i][j],3)\n",
    "\n",
    "    for l in range(0, len(df_names)):\n",
    "        participants_scores.append([df_names[l], scores_together[l]])\n",
    "         \n",
    "    return participants_scores\n",
    "\n",
    "def pair_labels_with_participants(labels, df_names):\n",
    "    participants_labels = []\n",
    "    i = 0\n",
    "    for i in range(0,len(df_names)):\n",
    "        participants_labels.append([df_names[i], labels[i]])\n",
    "    return participants_labels\n",
    "\n",
    "\n",
    "def get_participants_with_label(labels, label, testers_names):\n",
    "    indexes = np.where(labels == label)[0]\n",
    "    return list(map(lambda i: ALL_PARTICIPANTS[testers_names[i]], indexes))\n",
    "\n",
    "\n",
    "def transform_data(data):\n",
    "    data_array = []\n",
    "    for d in data:\n",
    "        data_array.append(d.data)\n",
    "        \n",
    "    return data_array\n",
    "\n",
    "\n",
    "def outliers_nd_sd(X_scores, paired_scores):\n",
    "# https://www.kdnuggets.com/2017/02/removing-outliers-standard-deviation-python.html\n",
    "# http://colingorrie.github.io/outlier-detection.html\n",
    "    final_list = []\n",
    "    final_list_temp = []\n",
    "    outliers = []\n",
    "    mean = np.mean(X_scores, axis=0)\n",
    "    sd = np.std(X_scores, axis=0)\n",
    "\n",
    "    #final_list = [x for x in paired_labels_orig if (x[1] > mean - 2 * sd)]\n",
    "    for x in paired_scores:\n",
    "        if(x[1] > mean - 2 * sd):\n",
    "            final_list_temp.append(x)\n",
    "        else:\n",
    "            outliers.append(x)\n",
    "    #final_list = [x for x in final_list if (x[1] < mean + 2 * sd)]\n",
    "    for x in final_list_temp:\n",
    "        if(x[1] < mean + 2 * sd):\n",
    "            final_list.append(x)\n",
    "        else:\n",
    "            outliers.append(x)\n",
    "            \n",
    "    return outliers\n",
    "\n",
    "def outliers_z_score(ys):\n",
    "    threshold = 3\n",
    "\n",
    "    mean_y = np.mean(ys)\n",
    "    stdev_y = np.std(ys)\n",
    "    z_scores = [(y - mean_y) / stdev_y for y in ys]\n",
    "    return np.where(np.abs(z_scores) > threshold)\n",
    "\n",
    "def loopRange(start, end):\n",
    "    return range(start, end+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "_________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCS \n",
    "Longest common subsequence - z dvoch stringov najde najdlhsu podpostupnost (to znamena, ze neberie do uvahy \"noisy\" znaky)\n",
    "\n",
    "Implementacia Lcs() prevzata z https://rosettacode.org/wiki/Longest_common_subsequence#Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcs(a, b):\n",
    "    lengths = [[0 for j in range(len(b)+1)] for i in range(len(a)+1)]\n",
    "    # row 0 and column 0 are initialized to 0 already\n",
    "    for i, x in enumerate(a):\n",
    "        for j, y in enumerate(b):\n",
    "            if x == y:\n",
    "                lengths[i+1][j+1] = lengths[i][j] + 1\n",
    "            else:\n",
    "                lengths[i+1][j+1] = max(lengths[i+1][j], lengths[i][j+1])\n",
    "    # read the substring out from the matrix\n",
    "    result = \"\"\n",
    "    x, y = len(a), len(b)\n",
    "    while x != 0 and y != 0:\n",
    "        if lengths[x][y] == lengths[x-1][y]:\n",
    "            x -= 1\n",
    "        elif lengths[x][y] == lengths[x][y-1]:\n",
    "            y -= 1\n",
    "        else:\n",
    "            assert a[x-1] == b[y-1]\n",
    "            result = a[x-1] + result\n",
    "            x -= 1\n",
    "            y -= 1\n",
    "    return result\n",
    "\n",
    "def get_LCS_features(participant, all_participants_features_names):\n",
    "    lcs_values = []\n",
    "    for tester in all_participants_features_names:\n",
    "        if(tester != participant):\n",
    "            lcs_values.append(len(lcs(ALL_PARTICIPANTS[participant].data[\"ScanpathShort\"], ALL_PARTICIPANTS[tester].data[\"ScanpathShort\"])))\n",
    "    \n",
    "    scanpath_len = len(ALL_PARTICIPANTS[participant].data[\"ScanpathShort\"])\n",
    "    lcs_max = np.max(lcs_values) / scanpath_len\n",
    "    lcs_mean = np.mean(lcs_values) / scanpath_len\n",
    "    lcs_min = np.min(lcs_values) / scanpath_len\n",
    "    \n",
    "    ALL_PARTICIPANTS[participant].set_data(\"MaxLcs\", lcs_max)\n",
    "    ALL_PARTICIPANTS[participant].set_data(\"MeanLcs\", lcs_mean)\n",
    "    ALL_PARTICIPANTS[participant].set_data(\"MinLcs\", lcs_min)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levensthein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LEV_features(participant, all_participants_features_names):\n",
    "    lev_values = []\n",
    "    for tester in all_participants_features_names:\n",
    "        if(tester != participant):\n",
    "            lev_values.append(distance.levenshtein(ALL_PARTICIPANTS[participant].data[\"ScanpathShort\"], ALL_PARTICIPANTS[tester].data[\"ScanpathShort\"]))\n",
    "    \n",
    "    scanpath_len = len(ALL_PARTICIPANTS[participant].data[\"ScanpathShort\"])\n",
    "    lev_max = np.max(lev_values)\n",
    "    lev_mean = np.mean(lev_values)\n",
    "    lev_min = np.min(lev_values)\n",
    "    \n",
    "    ALL_PARTICIPANTS[participant].set_data(\"MaxLev\", lev_max)\n",
    "    ALL_PARTICIPANTS[participant].set_data(\"MeanLev\", lev_mean)\n",
    "    ALL_PARTICIPANTS[participant].set_data(\"MinLev\", lev_min)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Time Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DTW(participant, all_participants_features_names):\n",
    "    dist = []\n",
    "    for m in all_participants_features_names:\n",
    "        if(m != participant):\n",
    "            distance, path = fastdtw(ALL_PARTICIPANTS[participant].fixationsToAoi, ALL_PARTICIPANTS[m].fixationsToAoi, dist=minkowski)\n",
    "            dist.append(distance)\n",
    "    \n",
    "    dtw_mean = np.mean(dist)\n",
    "    ALL_PARTICIPANTS[participant].set_data(\"DTW\", dtw_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RQA_features(participant):\n",
    "    clusteringDistanceThreshold = 70\n",
    "    timeDelayValue = 1\n",
    "    numTimeDelaySamples = 3\n",
    "    phaseSpaceClusteringThreshold = 0.5\n",
    "\n",
    "    results = {}\n",
    "    fixations = []\n",
    "    for fixation in participant.fixations:\n",
    "        fixations.append([int(fixation.x), int(fixation.y)])\n",
    "        #print(fixation)\n",
    "        # reoccurrences\n",
    "    #clusteringDistanceThreshold = int(parser.get('RQA', 'clusteringDistanceThreshold'))\n",
    "    matrix = ReoccurrenceFunctions.CreateReoccurrenceMatrix(fixations, clusteringDistanceThreshold=clusteringDistanceThreshold)\n",
    "    #print(len(matrix))\n",
    "    if(len(matrix) < 8):\n",
    "        participant.hasEnoughData = False\n",
    "        return False\n",
    "    #Added \n",
    "    #results[\"Name\"] = parti.name\n",
    "    results[\"Reoccurrence\"] = ReoccurrenceFunctions.getReoccurrence(matrix)\n",
    "    results[\"ReoccurrenceRate\"] = ReoccurrenceFunctions.getReoccurrenceRate(matrix)\n",
    "    results[\"ReoccurrenceDeterminism\"] = ReoccurrenceFunctions.getDeterminism(matrix)\n",
    "    results[\"ReoccurrenceLaminarity\"] = ReoccurrenceFunctions.getLaminarity(matrix)\n",
    "    results[\"ReoccurrenceCORM\"] = ReoccurrenceFunctions.getCORM(matrix)\n",
    "    \n",
    "\n",
    "    #reccurrences\n",
    "    #timeDelayValue = int(parser.get('RQA', 'timeDelayValue'))\n",
    "    #numTimeDelaySamples = int(parser.get('RQA', 'numTimeDelaySamples'))\n",
    "    #phaseSpaceClusteringThreshold = float(parser.get('RQA', 'phaseSpaceClusteringThreshold'))\n",
    "    fixationsXYPhaseSpaceData = RecurrenceFunctions.TimeDelayEmbedding(timeSeriesObservations=fixations,\n",
    "                                                                       delayStep=timeDelayValue,\n",
    "                                                                       delaySamples=numTimeDelaySamples)\n",
    "    recurrenceMatrixData = RecurrenceFunctions.CreateRecurrenceMatrix(phaseSpaceData=fixationsXYPhaseSpaceData,\n",
    "                                                                      clusteringDistanceThreshold= \tphaseSpaceClusteringThreshold );\n",
    "\n",
    "    results[\"Recurrence\"] = RecurrenceFunctions.getRecurrence(recurrenceMatrixData, numTimeDelaySamples);\n",
    "    results[\"RecurrenceRate\"] = RecurrenceFunctions.getRecurrenceRate(recurrenceMatrixData, numTimeDelaySamples);\n",
    "    (results[\"RecurrenceMeanX\"], results[\"RecurrenceMeanY\"]) = RecurrenceFunctions.getRecurrenceMean(recurrenceMatrixData, numTimeDelaySamples);\n",
    "    (results[\"RecurrenceStandardDeviationX\"],results[\"RecurrenceStandardDeviationY\"]) = RecurrenceFunctions.getRecurrenceStandardDeviation(recurrenceMatrixData,\n",
    "                                                                                             numTimeDelaySamples);\n",
    "    #reoccurrence[parti.name] = results\n",
    "   \n",
    "    for resultKey in results:\n",
    "        participant.set_data(resultKey, results[resultKey])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User features \n",
    "Vypocitaju sa crty pre participantov, okrem crt ktore si vyzaduju vsetky fixacie pre pozivatelov, napriklad RQA alebo parove porovnania ako DTW, LEV, LCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_features(dataset, _participant, fil = False):\n",
    "    print(\"Working on \" + _participant.data[\"Name\"])\n",
    "    # Vytiahnem data od konkretneho usera\n",
    "    d = dataset[dataset['ParticipantName'] == _participant.data[\"Name\"]]\n",
    "    # Vyfiltrujem data na konkretny task \n",
    "    # (neuvazujem Instrukcie alebo Questionare na konci - len ked sa zacalo a skoncilo nahravanie)\n",
    "    if fil:\n",
    "        d = dp.filter_users_data_to_task(d)\n",
    "    # Vyfiltrujem take fixacie ktore su NaN, teda neboli zaznamenane a nedalo by sa s nimi pracovat\n",
    "    d = dp.filter_users_fixations_wod(d)\n",
    "    \n",
    "    #_participant = Participant(tester)\n",
    "    #_participant = tester\n",
    "    \n",
    "    if(len(d) == 0):\n",
    "        _participant.hasEnoughData = False\n",
    "        return _participant \n",
    "    \n",
    "     # init\n",
    "    allFixations = []\n",
    "    # For saccade calculation and their speed, length\n",
    "    allFixationsWithTimestamp = []\n",
    "    allFixationsToAoi = []\n",
    "    scanpath = \"\"\n",
    "    scanpathShort = \"\"\n",
    "    scanpathLength = 0\n",
    "    scanpathShortLength = 0\n",
    "    numberOfAoisHits = 0\n",
    "    numberOfAoisHitsShort = 0\n",
    "    numberOfFixations = 0\n",
    "    fixationsDuration = 0\n",
    "    averageFixationDuration = 0\n",
    "    \n",
    "    averageSaccadeSpeed = []\n",
    "    averageSaccadeLength = []\n",
    "   \n",
    "\n",
    "    #_participant = Participant(tester)\n",
    "\n",
    "    i = 0\n",
    "    # Prechadzam jednotlive riadky\n",
    "    for index, row in d.iterrows():\n",
    "        distance = 0\n",
    "        speed = 0\n",
    "        fixationPoint = Point(row['FixationPointX (MCSpx)'], row['FixationPointY (MCSpx)'])\n",
    "        allFixations.append(fixationPoint)\n",
    "        allFixationsWithTimestamp.append([fixationPoint, row['RecordingTimestamp']])\n",
    "        \n",
    "        fixationsDuration += row['GazeEventDuration']\n",
    "        numberOfFixations += 1\n",
    "\n",
    "        for aoi in AOIS:\n",
    "            if AOIS[aoi].contains(fixationPoint):\n",
    "                scanpath += aoi\n",
    "                numberOfAoisHits += 1\n",
    "                allFixationsToAoi.append([fixationPoint.x, fixationPoint.y])\n",
    "                break\n",
    "                \n",
    "        if(i > 1):\n",
    "            distance = allFixationsWithTimestamp[i-1][0].distance(allFixationsWithTimestamp[i][0])\n",
    "            speed = distance / (allFixationsWithTimestamp[i][1] - allFixationsWithTimestamp[i-1][1])\n",
    "            averageSaccadeLength.append(distance)\n",
    "            averageSaccadeSpeed.append(speed)\n",
    "        \n",
    "        i += 1\n",
    "               \n",
    "    scanpathShort = ''.join(ch for ch, _ in itertools.groupby(scanpath))\n",
    "    numberOfAoisHitsShort = len(scanpathShort)\n",
    "    averageFixationDuration = ((fixationsDuration/numberOfFixations) / 1000) if numberOfFixations > 0 else 0\n",
    "    \n",
    "    _participant.set_data(\"Scanpath\", scanpath)\n",
    "    _participant.set_data(\"ScanpathShort\", scanpathShort)\n",
    "    _participant.set_data(\"NumberOfFixations\", numberOfFixations)\n",
    "    _participant.set_data(\"NumberOfAoisHits\", numberOfAoisHits)\n",
    "    _participant.set_data(\"NumberOfAoisHitsShort\", numberOfAoisHitsShort)\n",
    "    _participant.set_data(\"FixationsDuration\", fixationsDuration / 1000) # in seconds\n",
    "    _participant.set_data(\"AverageFixationDuration\", averageFixationDuration)\n",
    "    _participant.set_data(\"AverageSaccadeLength\", np.mean(np.array(averageSaccadeLength)) if len(averageSaccadeLength) > 0 else 0)\n",
    "    _participant.set_data(\"AverageSaccadeSpeed\", np.mean(np.array(averageSaccadeSpeed)) if len(averageSaccadeSpeed) > 0 else 0)\n",
    "\n",
    "    #_participant.scanpath = scanpath\n",
    "    #_participant.scanpath_short = ''.join(ch for ch, _ in itertools.groupby(scanpath))\n",
    "    #_participant.allFixationsDuration = duration\n",
    "    \n",
    "    _participant.fixations = allFixations\n",
    "    _participant.fixationsToAoi = np.array(allFixationsToAoi)\n",
    "    \n",
    "    \n",
    "    #ALL_PARTICIPANTS[tester] = _participant\n",
    "    \n",
    "    if(len(scanpathShort) > 1):\n",
    "        _participant.hasEnoughData = True\n",
    "        get_RQA_features(_participant)\n",
    "    else:\n",
    "        _participant.hasEnoughData = False\n",
    "        \n",
    "    return _participant\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________\n",
    "### Filtration\n",
    "Implementacia fitracii v metode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter based on the calibration values at the beginning of the experiment\n",
    "def calibration_filter(accur=0.85, precision=0.85):\n",
    "    \n",
    "    df_after_calibration = dvf.calibration_filter(DATASET, \"begin\", PATH_CALIB_DATA,  2, accur, precision)\n",
    "    testers_names_calib = df_after_calibration['ParticipantName'].unique()\n",
    "\n",
    "    #ALL_PARTICIPANTS_after_calibration = {}\n",
    "    dont_pass_calibration = list(set(ALL_PARTICIPANTS_NAMES) - set(testers_names_calib))\n",
    "\n",
    "    for tester in ALL_PARTICIPANTS:\n",
    "        if tester not in dont_pass_calibration:\n",
    "            ALL_PARTICIPANTS[tester].passCalibration = True\n",
    "            #ALL_PARTICIPANTS_after_calibration[tester] = ALL_PARTICIPANTS[tester]\n",
    "\n",
    "    print(str(len(ALL_PARTICIPANTS)) + \" -> \" + str(len(ALL_PARTICIPANTS) - len(dont_pass_calibration)))\n",
    "    return dont_pass_calibration\n",
    "\n",
    "\n",
    "# Filter dataset based on validation\n",
    "def validation_filter(percent = 25):\n",
    "    dataset_filtered = dvf.eyes_validity_filter(DATASET, 2, percent)\n",
    "    testers_names_validation = dataset_filtered['ParticipantName'].unique()\n",
    "\n",
    "    #ALL_PARTICIPANTS_after_validation = {}\n",
    "    dont_pass_validation = list(set(ALL_PARTICIPANTS_NAMES) - set(testers_names_validation))\n",
    "\n",
    "    for tester in ALL_PARTICIPANTS:\n",
    "        if tester not in dont_pass_validation:\n",
    "            ALL_PARTICIPANTS[tester].passValidation = True\n",
    "            #ALL_PARTICIPANTS_after_validation[tester] = ALL_PARTICIPANTS[tester]\n",
    "\n",
    "    print(str(len(ALL_PARTICIPANTS)) + \" -> \" + str(len(ALL_PARTICIPANTS) - len(dont_pass_validation)))\n",
    "    return dont_pass_validation\n",
    "    \n",
    "    \n",
    "# Filter users who don't complete task\n",
    "def completion_filter():\n",
    "    #ALL_PARTICIPANTS_who_complete = {}\n",
    "    for tester in ALL_PARTICIPANTS:\n",
    "        if tester not in DONT_COMPLETE_TASK:\n",
    "            ALL_PARTICIPANTS[tester].completeTask = True\n",
    "            #ALL_PARTICIPANTS_who_complete[tester] = ALL_PARTICIPANTS[tester]\n",
    "\n",
    "    print(str(len(ALL_PARTICIPANTS)) + \" -> \" + str(len(ALL_PARTICIPANTS) - len(DONT_COMPLETE_TASK)))\n",
    "\n",
    "    \n",
    "# Get users who pass\n",
    "def filter_participants(validationPercentage, calibration_check = True, validation_check = True, completion_check = True, accur=0.85, precision=0.85):\n",
    "    dont_pass_calibration = None\n",
    "    dont_pass_validation = None\n",
    "    \n",
    "    if(calibration_check):\n",
    "        print(\"Calibration filter: \")\n",
    "        dont_pass_calibration = calibration_filter(accur,precision)\n",
    "        \n",
    "    if(validation_check):\n",
    "        print(\"Validation filter: \")\n",
    "        dont_pass_validation = validation_filter(validationPercentage)\n",
    "        \n",
    "    if(completion_check):\n",
    "        print(\"Completion filter: \")\n",
    "        completion_filter()\n",
    "        \n",
    "    for tester in ALL_PARTICIPANTS:\n",
    "        if((ALL_PARTICIPANTS[tester].completeTask == completion_check) & (ALL_PARTICIPANTS[tester].passValidation == validation_check) & (ALL_PARTICIPANTS[tester].passCalibration == calibration_check)):\n",
    "            ALL_PARTICIPANTS_AFTER_COMPLETE_CHECK_NAMES.append(tester)\n",
    "    \n",
    "    print(\"Participants after all filtrations: \")\n",
    "    print(str(len(ALL_PARTICIPANTS)) + \" -> \" + str(len(ALL_PARTICIPANTS_AFTER_COMPLETE_CHECK_NAMES)))\n",
    "    \n",
    "    return dont_pass_calibration, dont_pass_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning\n",
    "_________\n",
    "Zhlukovacie algoritmy a LOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(participants_names, columns_to_drop = ['Name','Scanpath','ScanpathShort']):\n",
    "    \n",
    "    all_data = [ALL_PARTICIPANTS[tester].data for tester in participants_names]\n",
    "    \n",
    "    df_all_data = pd.DataFrame(all_data)\n",
    "    df_names = df_all_data['Name'].copy()\n",
    "    df_ = df_all_data.drop(columns=columns_to_drop)\n",
    "    df_final = df_.values.tolist()\n",
    "    X = preprocessing.scale(df_final)\n",
    "\n",
    "    return X, df_names, df_all_data, df_\n",
    "\n",
    "def split_data_to_2_clusters(labels, df_names, df_all_data):\n",
    "    gpw_0_names = [n.data[\"Name\"] for n in get_participants_with_label(labels, 0, df_names)]\n",
    "    gpw_1_names = [n.data[\"Name\"] for n in get_participants_with_label(labels, 1, df_names)]\n",
    "    df_cluster_0 = df_all_data.loc[df_all_data['Name'].isin(gpw_0_names)]\n",
    "    df_cluster_1 = df_all_data.loc[df_all_data['Name'].isin(gpw_1_names)]\n",
    "    \n",
    "    return df_cluster_0, df_cluster_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureImportance(X, labels, df_, estimators = 200, typeOfBar = 1):\n",
    "    # Build a classification task using 3 informative features\n",
    "    # X, y = make_classification(n_samples=1000,\n",
    "    #                            n_features=10,\n",
    "    #                            n_informative=3,\n",
    "    #                            n_redundant=0,\n",
    "    #                            n_repeated=0,\n",
    "    #                            n_classes=2,\n",
    "    #                            random_state=0,\n",
    "    #                            shuffle=False)\n",
    "\n",
    "    # Build a forest and compute the feature importances\n",
    "    #forest = ExtraTreesClassifier(n_estimators=1000)\n",
    "    forest = RandomForestClassifier(n_estimators=estimators)\n",
    "\n",
    "    forest.fit(X, labels)\n",
    "    #forest.fit(X, y)\n",
    "    importances = forest.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(X.shape[1]):\n",
    "        print(\"%d. feature %d, %s (%f)\" % (f + 1, indices[f], df_.columns[indices[f]], importances[indices[f]]))\n",
    "\n",
    "    # Plot the feature importances of the forest\n",
    "    if(typeOfBar == 1):\n",
    "        plt.figure()\n",
    "        plt.title(\"Feature importances\")\n",
    "        #plt.bar(range(X.shape[1]), importances[indices],\n",
    "               #color=\"r\", yerr=std[indices], align=\"center\")\n",
    "        #plt.xticks(range(X.shape[1]), indices)\n",
    "        #plt.xlim([-1, X.shape[1]])\n",
    "        plt.barh(range(X.shape[1]), importances[indices], color=\"r\", align=\"center\")\n",
    "        plt.yticks(range(X.shape[1]), [df_.columns[indices[f]] for f in range(X.shape[1])])\n",
    "        plt.ylim([-1, X.shape[1]])\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.gcf().subplots_adjust(left=0.4)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        d_crt = pd.DataFrame(importances[indices])\n",
    "        d_crt.columns=[\"Dôležitost črty\"]\n",
    "        ax = d_crt.plot(kind='bar', color='red', figsize=(10,10))\n",
    "        ax.set_xticklabels([df_.columns[indices[f]] for f in range(X.shape[1])], rotation='vertical', fontsize=10)\n",
    "        \n",
    "    return forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Outlier Factor\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/neighbors/plot_lof_outlier_detection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LOF(X, df_names, k_neighbors = 15, m_metric = \"minkowski\", with_plot = False, with_text = True, x=-10, y=25, name_index = 6):\n",
    "    clf = LocalOutlierFactor(n_neighbors=k_neighbors, contamination=0.05, metric=m_metric)\n",
    "    # use fit_predict to compute the predicted labels of the training samples\n",
    "    # (when LOF is used for outlier detection, the estimator has no predict,\n",
    "    # decision_function and score_samples methods).\n",
    "    y_pred = clf.fit_predict(X)\n",
    "    X_scores = clf.negative_outlier_factor_\n",
    "\n",
    "    if(with_plot == True):\n",
    "        pca_lof = PCA(n_components=2).fit(X)\n",
    "        plot_lof = pca_lof.transform(X)\n",
    "\n",
    "        plt.title(\"Local Outlier Factor (LOF)\")\n",
    "        plt.scatter(plot_lof[:, 0], plot_lof[:, 1], color='k', s=3., label='Data points')\n",
    "        # plot circles with radius proportional to the outlier scores\n",
    "        radius = (X_scores.max() - X_scores) / (X_scores.max() - X_scores.min())\n",
    "        plt.scatter(plot_lof[:, 0], plot_lof[:, 1], s=1000 * radius, edgecolors='r',\n",
    "                    facecolors='none', label='Outlier scores')\n",
    "        plt.axis('tight')\n",
    "        plt.xlim((x, y))\n",
    "        plt.ylim((x, y))\n",
    "        legend = plt.legend(loc='upper left')\n",
    "        legend.legendHandles[0]._sizes = [10]\n",
    "        legend.legendHandles[1]._sizes = [20]\n",
    "        \n",
    "        if(with_text == True):\n",
    "            for i,name in enumerate(df_names):\n",
    "                x = plot_lof[i][0]\n",
    "                y = plot_lof[i][1]\n",
    "                plt.text(x+0.2, y+0.05, name[name_index:], fontsize=8)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    return X_scores,clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_KMEANS(X, df_names, num_clusters = 2, name_Index = 1, with_plot = True, with_text = True, colors = COLORS):\n",
    "    kmeans = KMeans(n_clusters=num_clusters, init=\"k-means++\", algorithm=\"auto\", n_init=15, random_state=None).fit(X)\n",
    "\n",
    "    labels = kmeans.labels_\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "    print(\"Silhouette Coefficient: %0.3f\" % silhouette_score(X, labels))\n",
    "    \n",
    "    if(with_plot == True):\n",
    "        plt.figure()\n",
    "        # Vytvorenie a nafitovanie PCA modelu\n",
    "        pca = PCA(n_components=2).fit(X)\n",
    "        plot = pca.transform(X)\n",
    "        plt.scatter(x=plot[:,0], y=plot[:,1], c=[colors[l_ + int(1)] for l_ in labels], s=50)  \n",
    "        plt.xlim((-6, 12))\n",
    "        plt.ylim((-6, 12))\n",
    "        \n",
    "        if(with_text == True):\n",
    "            for i,name in enumerate(df_names):\n",
    "                x = plot[i][0]\n",
    "                y = plot[i][1]\n",
    "                plt.text(x+0.2, y+0.05, name[name_Index:], fontsize=8)\n",
    "                \n",
    "        # Zobrazenie PCA 2D modelu\n",
    "        title = \"Number of clusters: %d\" % n_clusters_\n",
    "        plt.title(title)\n",
    "        \n",
    "    return labels, kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-medoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_KMEDOIDS(X, df, init_medoids = [10, 30], name_Index = 1, with_plot = True, with_text = True, colors = COLORS):\n",
    " \n",
    "    initial_medoids = init_medoids\n",
    "\n",
    "    # create instance of K-Medoids algorithm\n",
    "    kmedoids_instance = kmedoid.kmedoids(X, initial_medoids)\n",
    "\n",
    "    # run cluster analysis and obtain results\n",
    "    kmedoids_instance.process();\n",
    "    clusters = kmedoids_instance.get_clusters()\n",
    "    labels = clusters_to_labels(X, clusters)\n",
    "    \n",
    "    print(\"Silhouette Coefficient: %0.3f\" % silhouette_score(X, labels))\n",
    "    \n",
    "    if(with_plot == True):\n",
    "        plt.figure()\n",
    "        # Vytvorenie a nafitovanie PCA modelu\n",
    "        pca = PCA(n_components=2).fit(X)\n",
    "        plot = pca.transform(X)\n",
    "        plt.scatter(x=plot[:,0], y=plot[:,1], c=[colors[l_ + int(1)] for l_ in labels], s=50)  \n",
    "        \n",
    "        if(with_text == True):\n",
    "            for i,name in enumerate(df_names):\n",
    "                x = plot[i][0]\n",
    "                y = plot[i][1]\n",
    "                plt.text(x+0.2, y+0.05, name[name_Index:], fontsize=8)\n",
    "                \n",
    "        # Zobrazenie PCA 2D modelu\n",
    "        title = \"Number of clusters: %d\" % len(clusters)\n",
    "        plt.title(title)\n",
    "        \n",
    "    return labels, kmedoids_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of user's features\n",
    "\n",
    "Participant musi mat dostatok dat pre vypocet crt\n",
    "\n",
    "Vypocet vsetkych crt pre vsetkych participantov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_participants_features(w = True, fil = False):\n",
    "    ALL_PARTICIPANTS_FEATURES_NAMES = []\n",
    "    for tester in ALL_PARTICIPANTS_AFTER_COMPLETE_CHECK_NAMES:\n",
    "        ALL_PARTICIPANTS[tester] = get_user_features(DATASET, ALL_PARTICIPANTS[tester],fil)\n",
    "        if(ALL_PARTICIPANTS[tester].hasEnoughData == True):\n",
    "            ALL_PARTICIPANTS_FEATURES_NAMES.append(tester)\n",
    "        else:\n",
    "            DONT_HAVE_ENOUGH_DATA.append(tester)\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    print(\"Other features\")\n",
    "    if(w == True):\n",
    "        ### Get DTW, Levensthein, LCS for each participant \n",
    "        # We must have valid and enough data - all fixations and scanpath ready\n",
    "        for tester in ALL_PARTICIPANTS_FEATURES_NAMES:\n",
    "            print(\"Working on \" + tester)\n",
    "            get_DTW(tester, ALL_PARTICIPANTS_FEATURES_NAMES)\n",
    "            get_LEV_features(tester, ALL_PARTICIPANTS_FEATURES_NAMES)\n",
    "            get_LCS_features(tester, ALL_PARTICIPANTS_FEATURES_NAMES)\n",
    "            \n",
    "    print(\"Participants after feature calculations: \")\n",
    "    print(len(ALL_PARTICIPANTS_AFTER_COMPLETE_CHECK_NAMES) - len(DONT_HAVE_ENOUGH_DATA))\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    \n",
    "    return ALL_PARTICIPANTS_FEATURES_NAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See dropouts (participants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overall_dropouts(all_participants_names):\n",
    "    print(\"Overal dropouts: \")\n",
    "    overall_dropouts = list(set(all_participants_names) - set(ALL_PARTICIPANTS_FEATURES_NAMES))\n",
    "    df_cleaned = DATASET\n",
    "    for tester in overall_dropouts:\n",
    "        df_cleaned = df_cleaned[df_cleaned['ParticipantName'] != tester]\n",
    "    \n",
    "    if(DONT_PASS_CALIBRATION != None):\n",
    "        print(\"Participants who didn't pass through calibration = \" + str(len(DONT_PASS_CALIBRATION)) + \" -> \" + str(DONT_PASS_CALIBRATION))\n",
    "    if(DONT_COMPLETE_TASK != None):\n",
    "        print(\"Participants who didn't complete task = \" + str(len(DONT_COMPLETE_TASK)) + \" -> \" + str(DONT_COMPLETE_TASK))\n",
    "    if(DONT_PASS_VALIDATION != None):\n",
    "        print(\"Participants who didn't pass through validation = \" + str(len(DONT_PASS_VALIDATION)) + \" -> \" + str(DONT_PASS_VALIDATION))\n",
    "    if(DONT_HAVE_ENOUGH_DATA != None):\n",
    "        print(\"Participants who didn't have enough data = \" + str(len(DONT_HAVE_ENOUGH_DATA)) + \" -> \" + str(DONT_HAVE_ENOUGH_DATA))\n",
    "    print(\"------------------\")\n",
    "    print(\"All Participants who didnt pass = \" + str(len(overall_dropouts)) + \" -> \" + str(overall_dropouts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOF in iterations and finding outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lof_iterations(X,df_names_lof, lower, upper, metric=\"minkowski\"):\n",
    "    #metric = \"correlation\"\n",
    "    #metric = \"minkowski\"\n",
    "    all_X_scores = []\n",
    "    #X_ = X #X_cluster_0/1 # X\n",
    "    #df_names_lof = df_names # df_names_cluster_0/1 # df_names\n",
    "    for k in loopRange(lower,upper):\n",
    "        sc, lf = run_LOF(X ,df_names_lof, k, metric)\n",
    "        all_X_scores.append(sc)\n",
    "\n",
    "    pso = pair_multiple_scores_with_participants(all_X_scores, np.array(df_names_lof))\n",
    "    fr = pd.DataFrame(pso)\n",
    "    pso_max = []\n",
    "    scoores = []\n",
    "    for name in pso:\n",
    "        pso_max.append([name[0],np.min(name[1])])\n",
    "        scoores.append(np.min(name[1]))\n",
    "\n",
    "    #pso_max\n",
    "    #pso_max_sorted = sorted(pso_max, key=lambda x: x[1])\n",
    "    #pddf1 = pd.DataFrame(pso_max_sorted)\n",
    "    sorted_scores = pd.DataFrame(pso_max).sort_values(1)\n",
    "    return scoores,pso_max, sorted_scores\n",
    "\n",
    "def get_outliers_based_on_score(scoores,pso_max):\n",
    "    outliers = outliers_nd_sd(np.array(scoores), pso_max)\n",
    "    outliers = [x[0] for x in outliers]\n",
    "    print(\"Outliers in data -> \" + str(outliers))\n",
    "    return outliers\n",
    "\n",
    "def get_outliers(X, df_names, cluster=False,metric=\"correlation\", num_of_outliers_to_find = 2):\n",
    "    #metric = \"correlation\"\n",
    "    #metric = \"minkowski\"\n",
    "    all_X_scores = []\n",
    "    X_ = X #X_cluster_0/1 # X\n",
    "    df_names_lof = df_names # df_names_cluster_0/1 # df_names\n",
    "    \n",
    "    bc = len(df_names_cluster_0) if (len(df_names_cluster_0) > len(df_names_cluster_1)) else len(df_names_cluster_1)\n",
    "    \n",
    "    # dolny a horny index\n",
    "    if(cluster):\n",
    "        l = int(len(df_names) - (len(df_names) / 4))\n",
    "    else:\n",
    "        l = bc - num_of_outliers_to_find\n",
    "        \n",
    "    h = len(df_names) - num_of_outliers_to_find\n",
    "    \n",
    "    print(\"** \" + metric + \" **\")\n",
    "    scoores,pso_max, sorted_scores = run_lof_iterations(X,df_names_lof, l, h, metric)\n",
    "    outliers = get_outliers_based_on_score(scoores,pso_max)\n",
    "    \n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting participant's fixations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_participant_fixations_to_aoi(testerName, one = False, testerName2 = \"\", with_image = False, imgPath = \"\"):\n",
    "    if(with_image == True):\n",
    "        img = plt.imread(imgPath)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(img)\n",
    "        \n",
    "    if(one == False):\n",
    "        plt.plot(ALL_PARTICIPANTS[testerName].fixationsToAoi[:,0], ALL_PARTICIPANTS[testerName].fixationsToAoi[:,1], c='blue')\n",
    "        plt.plot(ALL_PARTICIPANTS[testerName2].fixationsToAoi[:,0], ALL_PARTICIPANTS[testerName2].fixationsToAoi[:,1], c='red')\n",
    "    else:\n",
    "        plt.plot(ALL_PARTICIPANTS[testerName].fixationsToAoi[:,0], ALL_PARTICIPANTS[testerName].fixationsToAoi[:,1])\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "def plot_participant_fixations(testerName, one = False, testerName2 = \"\",  with_image = False, imgPath = \"\"):\n",
    "    if(with_image == True):\n",
    "        img = plt.imread(imgPath)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(img)\n",
    "        \n",
    "    if(one == False):\n",
    "        plt.plot([a.x for a in ALL_PARTICIPANTS[testerName].fixations], [b.y for b in ALL_PARTICIPANTS[testerName].fixations], c='blue')\n",
    "        plt.plot([a.x for a in ALL_PARTICIPANTS[testerName2].fixations], [b.y for b in ALL_PARTICIPANTS[testerName2].fixations], c='red')\n",
    "    else:\n",
    "        plt.plot([a.x for a in ALL_PARTICIPANTS[testerName].fixations], [b.y for b in ALL_PARTICIPANTS[testerName].fixations])\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save / load all main variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_variables(TASK_NUMBER, path):\n",
    "    with open(path + TASK_NUMBER + '/ALL_PARTICIPANTS', 'wb') as f:\n",
    "        pickle.dump(ALL_PARTICIPANTS, f)\n",
    "        f.close()\n",
    "    with open(path + TASK_NUMBER + '/ALL_PARTICIPANTS_FEATURES_NAMES', 'wb') as f:\n",
    "        pickle.dump(ALL_PARTICIPANTS_FEATURES_NAMES, f)\n",
    "        f.close()\n",
    "    with open(path + TASK_NUMBER + '/ALL_PARTICIPANTS_NAMES', 'wb') as f:\n",
    "        pickle.dump(ALL_PARTICIPANTS_NAMES, f)\n",
    "        f.close()\n",
    "    with open(path + TASK_NUMBER + '/ALL_PARTICIPANTS_AFTER_COMPLETE_CHECK_NAMES', 'wb') as f:\n",
    "        pickle.dump(ALL_PARTICIPANTS_AFTER_COMPLETE_CHECK_NAMES, f)\n",
    "        f.close()\n",
    "    with open(path + TASK_NUMBER + '/DONT_HAVE_ENOUGH_DATA', 'wb') as f:\n",
    "        pickle.dump(DONT_HAVE_ENOUGH_DATA, f)\n",
    "        f.close()\n",
    "    with open(path + TASK_NUMBER + '/DONT_COMPLETE_TASK', 'wb') as f:\n",
    "        pickle.dump(DONT_COMPLETE_TASK, f)\n",
    "        f.close()\n",
    "    with open(path + TASK_NUMBER + '/DONT_PASS_CALIBRATION', 'wb') as f:\n",
    "        pickle.dump(DONT_PASS_CALIBRATION, f)\n",
    "        f.close()\n",
    "    with open(path + TASK_NUMBER + '/DONT_PASS_VALIDATION', 'wb') as f:\n",
    "        pickle.dump(DONT_PASS_VALIDATION, f)\n",
    "        f.close()\n",
    "        \n",
    "def load_variables(TASK_NUMBER, path):\n",
    "    infile = open(path+TASK_NUMBER + '/ALL_PARTICIPANTS','rb')\n",
    "    ALL_PARTICIPANTS = pickle.load(infile)\n",
    "    infile.close()\n",
    "    \n",
    "    infile = open(path + TASK_NUMBER+ '/ALL_PARTICIPANTS_FEATURES_NAMES','rb')\n",
    "    ALL_PARTICIPANTS_FEATURES_NAMES = pickle.load(infile)\n",
    "    infile.close()\n",
    "    \n",
    "    infile = open(path+TASK_NUMBER+ '/ALL_PARTICIPANTS_NAMES','rb')\n",
    "    ALL_PARTICIPANTS_NAMES = pickle.load(infile)\n",
    "    infile.close()\n",
    "    \n",
    "    infile = open(path+TASK_NUMBER+ '/ALL_PARTICIPANTS_AFTER_COMPLETE_CHECK_NAMES','rb')\n",
    "    ALL_PARTICIPANTS_AFTER_COMPLETE_CHECK_NAMES = pickle.load(infile)\n",
    "    infile.close()\n",
    "    \n",
    "    infile = open(path+TASK_NUMBER+ '/DONT_HAVE_ENOUGH_DATA','rb')\n",
    "    DONT_HAVE_ENOUGH_DATA = pickle.load(infile)\n",
    "    infile.close()\n",
    "    \n",
    "    infile = open(path+TASK_NUMBER+ '/DONT_COMPLETE_TASK','rb')\n",
    "    DONT_COMPLETE_TASK = pickle.load(infile)\n",
    "    infile.close()\n",
    "    \n",
    "    infile = open(path+TASK_NUMBER+ '/DONT_PASS_CALIBRATION','rb')\n",
    "    DONT_PASS_CALIBRATION = pickle.load(infile)\n",
    "    infile.close()\n",
    "    \n",
    "    infile = open(path+TASK_NUMBER+ '/DONT_PASS_VALIDATION','rb')\n",
    "    DONT_PASS_VALIDATION = pickle.load(infile)\n",
    "    infile.close()\n",
    "   \n",
    "    return ALL_PARTICIPANTS,ALL_PARTICIPANTS_FEATURES_NAMES, ALL_PARTICIPANTS_NAMES, \\\n",
    "    ALL_PARTICIPANTS_AFTER_COMPLETE_CHECK_NAMES,DONT_HAVE_ENOUGH_DATA,DONT_COMPLETE_TASK,DONT_PASS_CALIBRATION,DONT_PASS_VALIDATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Funkcie exluzivne potrebne len pre MSNV\n",
    "def parse_tsv_by_screen():\n",
    "    columns = data_mmd.columns\n",
    "    data = {\n",
    "        3 : pd.DataFrame(columns=columns),\n",
    "        5 : pd.DataFrame(columns=columns),\n",
    "        9 : pd.DataFrame(columns=columns),\n",
    "        11 : pd.DataFrame(columns=columns),\n",
    "        18 : pd.DataFrame(columns=columns),\n",
    "        20 : pd.DataFrame(columns=columns),\n",
    "        27 : pd.DataFrame(columns=columns),\n",
    "        28 : pd.DataFrame(columns=columns),\n",
    "        30 : pd.DataFrame(columns=columns),\n",
    "        60 : pd.DataFrame(columns=columns),\n",
    "        62 : pd.DataFrame(columns=columns),\n",
    "        66 : pd.DataFrame(columns=columns),\n",
    "        72 : pd.DataFrame(columns=columns),\n",
    "        74 : pd.DataFrame(columns=columns),\n",
    "        76 : pd.DataFrame(columns=columns)\n",
    "    }\n",
    "    \n",
    "    for i in range(0, len(gaze_exports_names)):\n",
    "        participant_seg_files = pd.read_csv(path + segfiles_path + seg_files_names[i], low_memory=False, sep=\",\")\n",
    "        participant = f = pd.read_csv(path + gazes_path + gaze_exports_names[i], low_memory=False, sep=\"\\t\")\n",
    "        print(\"Working on \" + participant.iloc[0]['ParticipantName'])\n",
    "        for ide,row in participant_seg_files.iterrows():\n",
    "            mmd_id = row['mmd_id']\n",
    "            start = row['start']\n",
    "            end = row['end']\n",
    "            print(\"--\" + str(mmd_id))\n",
    "\n",
    "            data_mmd = participant[(participant['RecordingTimestamp'] >= start) & (participant['RecordingTimestamp'] <= end)]\n",
    "            data[mmd_id] = data[mmd_id].append(data_mmd)\n",
    "        print(\"--------------------------\")  \n",
    "        i = i + 1\n",
    "        \n",
    "    for d in data:\n",
    "        data[d].to_csv(path + data_by_screen_path + str(d) +\".tsv\",sep=\"\\t\")\n",
    "        \n",
    "        \n",
    "def load_aois(aois_index):\n",
    "    all_aois = {}\n",
    "\n",
    "    aois_columns = [\"Typ\",\"1\",\"2\",\"3\",\"4\"]\n",
    "    aois_file = pd.read_csv(path + aois_path + aois_files_names[aois_index], sep=\"\\t\").iloc[:, : 5]\n",
    "    aois_file.columns = aois_columns\n",
    "\n",
    "    i = 0\n",
    "    for index, row in aois_file.iterrows():\n",
    "        a = Point(float(aois_file.iloc[i][1].split(\",\")[0]), float(aois_file.iloc[i][1].split(\",\")[1]))\n",
    "        b = Point(float(aois_file.iloc[i][2].split(\",\")[0]), float(aois_file.iloc[i][2].split(\",\")[1]))\n",
    "        c = Point(float(aois_file.iloc[i][3].split(\",\")[0]), float(aois_file.iloc[i][3].split(\",\")[1]))\n",
    "        d = Point(float(aois_file.iloc[i][4].split(\",\")[0]), float(aois_file.iloc[i][4].split(\",\")[1]))\n",
    "\n",
    "        poly = Polygon(((a.x,a.y),(b.x,b.y),(c.x,c.y),(d.x,d.y)))\n",
    "\n",
    "        all_aois[aois_names[i]] = poly\n",
    "\n",
    "        i = i + 1\n",
    "        \n",
    "    return all_aois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_sorted_by(df_all_data, value=\"FixationsDuration\"):\n",
    "    sorted = df_all_data.sort_values([value])\n",
    "    return sorted[[value, \"Name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation_matrix(df_dropped_columns):\n",
    "    corr = df_dropped_columns.corr()\n",
    "    sns.set(rc={'figure.figsize':(8,6)})\n",
    "    sns.heatmap(corr, \n",
    "                xticklabels=corr.columns.values,\n",
    "                yticklabels=corr.columns.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
